import os
import pandas as pd
import logging
from config import DATASETS_CONFIG, HEADLINE_CONTENT_CONFIG
# The following import is needed for the default path logic
from data.clickbait17.clickbait17_utils import get_dataset_folder, create_combined_headline

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def _binarise(score: float, threshold: float = 0.5) -> int:
    """Converts a continuous clickbait score to a binary label."""
    return 1 if score >= threshold else 0

def prepare_headline_classifier_dataset_from_csv(input_csv_dir: str = None, output_path: str = None):
    """
    Creates datasets for HeadlineClassifier from pre-existing Clickbait17 CSV files.

    It reads the CSV files generated by the main data preparation script,
    and transforms them into the 'headline' and 'clickbait' format.
    """
    logger.info("\n--- Preparing datasets for HeadlineClassifier from Clickbait17 CSVs ---")

    # Define input and output paths
    if input_csv_dir is None:
        # Assume a default location based on the original prepare script's output
        default_tokenizer = HEADLINE_CONTENT_CONFIG["tokenizer_name"]
        input_csv_dir = get_dataset_folder(default_tokenizer)
        logger.info(f"Input CSV directory not provided, defaulting to: {input_csv_dir}")

    if output_path is None:
        output_path = os.path.join("data", "headline_classifier_dataset")

    os.makedirs(output_path, exist_ok=True)
    logger.info(f"Output directory set to: {output_path}")

    subsets = {
        "train": DATASETS_CONFIG["train_suffix"],
        "validation": DATASETS_CONFIG["validation_suffix"],
        "test": DATASETS_CONFIG["test_suffix"]
    }

    # Process each subset (train, validation, test)
    for subset_key, subset_name in subsets.items():
        output_filename = os.path.join(output_path, f"clickbait17_headline_{subset_key}.csv")

        if os.path.exists(output_filename):
            logger.info(f"Dataset for '{subset_key}' already exists at {output_filename}. Skipping.")
            continue

        # Path to the input CSV file for the current subset
        input_csv_filename = os.path.join(input_csv_dir, f"{DATASETS_CONFIG['dataset_headline_content_name']}_{subset_name}.csv")

        if not os.path.exists(input_csv_filename):
            logger.error(f"Input CSV not found: {input_csv_filename}. Please ensure this file exists. You may need to run the main 'clickbait17_prepare.py' script first.")
            continue

        logger.info(f"Creating dataset for '{subset_key}' from {input_csv_filename}...")

        # Step 1: Load the original Clickbait17 data from the CSV file
        try:
            original_df = pd.read_csv(input_csv_filename)
            if original_df.empty:
                logger.warning(f"The input CSV {input_csv_filename} is empty. Skipping.")
                continue
        except Exception as e:
            logger.error(f"Failed to read CSV file {input_csv_filename}: {e}")
            continue

        # Step 2: Create the new DataFrame with the required format
        logger.info(f"Transforming data for '{subset_key}'...")
        headline_classifier_df = pd.DataFrame()
        # Use 'post' column as 'headline'
        headline_classifier_df['headline'] = create_combined_headline(original_df)
        # Binarize 'clickbait_score' into 'clickbait'
        headline_classifier_df['clickbait'] = original_df['clickbait_score'].apply(_binarise)

        # Step 3: Save the new DataFrame to a CSV file
        headline_classifier_df.to_csv(output_filename, index=False)
        logger.info(f"Successfully saved '{subset_key}' dataset to {output_filename}")

    logger.info("\n--- Dataset preparation for HeadlineClassifier complete. ---")

if __name__ == "__main__":
    # This script now assumes that the CSV files from the original
    # clickbait17_prepare.py script already exist.
    prepare_headline_classifier_dataset_from_csv()